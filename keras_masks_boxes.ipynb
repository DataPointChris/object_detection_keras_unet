{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "https://www.kaggle.com/christofhenkel/keras-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# CURRENTLY WORKING WITH TENSORFLOW\n",
    "# \n",
    "# PRIORITIES:\n",
    "# 1. refactor code to better groupings\n",
    "# 2. where do the functions go\n",
    "# 3. grab validation data\n",
    "#\n",
    "#\n",
    "# If you are a potential EMPLOYER looking over my code, THANK YOU!\n",
    "# I love to hear comments and critiques about being a better programmer,\n",
    "# data engineer, and data scientist.  \n",
    "# If you found this code useful or learned something, YAY!\n",
    "# \n",
    "# LinkedIn: /in/chris-birch\n",
    "# Portfolio: www.datapointchris.com/portfolio\n",
    "# Website: www.datapointchris.com\n",
    "#\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================ FOR RUNNING ON THE MACBOOK PRO\n",
    "\n",
    "# # DONT LEAVE COMMENTED CODE IN THE FINAL CODE. WOULD HAVE TO FIND OS IF NECESSARY BUT NOT\n",
    "\n",
    "# ## DO THIS BEFORE IMPORTING KERAS OR TENSOR TO USE PLAIDML\n",
    "# import plaidml.keras\n",
    "# plaidml.keras.install_backend()\n",
    "\n",
    "# # Help MacOS be able to use Keras\n",
    "# import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "# # Gets rid of the processor warning.\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, Input, BatchNormalization, Lambda\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, concatenate, Concatenate, Conv2DTranspose, UpSampling2D\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# from tqdm.keras import TqdmCallback\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import skimage.io\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1bc8f9908ff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mpretrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'testing_model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mplot_epoch_val_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mearly_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATA_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# =========================================================================Parameters, paths and variables\n",
    "# =========================================================================Parameters, paths and variables\n",
    "# =========================================================================Parameters, paths and variables\n",
    "# hard coded for now\n",
    "\n",
    "train_image_dir = '../images/network_training/build/0/'\n",
    "train_mask_dir = '../images/network_training/mask/0/'\n",
    "test_image_dir = '../images/network_training/test/0/'\n",
    "data_dir = '../data/'\n",
    "\n",
    "# images to split and feed into the generator\n",
    "# decrease this number if running out of GPU memory\n",
    "# works with 4 on 11GB GPU\n",
    "images_per_batch = 1\n",
    "\n",
    "# val split from training set\n",
    "train_val_split_size = .1\n",
    "\n",
    "\n",
    "# IMAGE SPLITTER PARAMS #\n",
    "\n",
    "split_rows = 20\n",
    "split_cols = 20\n",
    "img_resize_width = 4800\n",
    "img_resize_height = 4800\n",
    "\n",
    "\n",
    "# MODEL PARAMS #\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 1\n",
    "pretrained_model = False\n",
    "model_name = 'testing_model'\n",
    "model_path = os.path.join(DATA_DIR, model_name + '.h5')\n",
    "plot_epoch_val_images = True\n",
    "early_stop = EarlyStopping(patience=5, verbose=1)\n",
    "check_point = ModelCheckpoint(os.path.join(DATA_DIR, model_name + '.hdf5'),\n",
    "                              save_best_only=True,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir_images = glob.glob(TRAIN_IMAGE_DIR + '*')\n",
    "train_filenames = [os.path.basename(x) for x in train_dir_images]\n",
    "\n",
    "test_dir_images = glob.glob(TEST_IMAGE_DIR + '*')\n",
    "test_filenames = [os.path.basename(x) for x in test_dir_images]\n",
    "\n",
    "# create batch sets of 'images_per_batch' size\n",
    "training_sets = [train_filenames[i :i + images_per_batch] for i in range(0,len(train_filenames),images_per_batch)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValPlotCallback(Callback):\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        print('VALIDATION IMAGES')\n",
    "        X_val_pred = model.predict(X_val, verbose=1, batch_size=batch_size)\n",
    "        X_val_pred_mask = (X_val_pred > 0.5).astype(np.uint8)\n",
    "        plot_predictions(original=X_val,\n",
    "                         predicted=X_val_pred,\n",
    "                         predicted_mask=X_val_pred_mask,\n",
    "                         ground_truth=y_val,\n",
    "                         repeat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(image_dir, gray=False):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    if gray is False:\n",
    "        batch = cv2.imread(image_dir + process_img).astype(np.uint8)\n",
    "    else:\n",
    "        batch = cv2.imread(image_dir + process_img, 0).astype(np.uint8)\n",
    "\n",
    "    split = image_splitter(batch,\n",
    "                           num_col_splits=split_cols,\n",
    "                           num_row_splits=split_rows,\n",
    "                           resize=resize,\n",
    "                           resize_height=img_resize_height,\n",
    "                           resize_width=img_resize_width)\n",
    "\n",
    "    vstacked_array = (np.vstack(split)/255).astype(np.float32)\n",
    "    if gray is False:\n",
    "        vstacked_array = np.expand_dims(vstacked_array, axis=3)  # grayscale\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_and_mem(array):\n",
    "    \"\"\"Prints the shape and memory size of an array\"\"\"\n",
    "    print(f'Shape: {array.shape}')\n",
    "    print(f'Size: {round(array.itemsize * array.size / 1024 / 1024 / 1024, 3)} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def image_splitter(image, num_col_splits, num_row_splits, resize=False, resize_width=None, resize_height=None):\n",
    "    \"\"\"\n",
    "    Splits an image into 'num_col_splits' X 'num_row_splits'\n",
    "    Resize by setting resize=True and specifying 'resize_width' and 'resize_height'\n",
    "    Returns array of images arranged from left -> right, top -> bottom\n",
    "    \"\"\"\n",
    "    if resize:\n",
    "        image = cv2.resize(image, (resize_width, resize_height))\n",
    "    \n",
    "    width = image.shape[0] \n",
    "    height = image.shape[1]\n",
    "    imglist = []\n",
    "\n",
    "    for startpoint in np.linspace(0,width,num_col_splits, endpoint=False):\n",
    "        endpoint=startpoint + (width / num_col_splits)\n",
    "\n",
    "        for startp2 in np.linspace(0,height,num_row_splits, endpoint=False):\n",
    "            endp2=startp2 + (height / num_row_splits)\n",
    "\n",
    "            imglist.append(image[int(startp2):int(endp2), int(startpoint):int(endpoint)]\n",
    "                        )\n",
    "    return np.array(imglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_tester(original):\n",
    "    \"\"\"Shows original images in array\"\"\"\n",
    "    for _ in range(4):\n",
    "        ix = np.random.randint(0, original.shape[0])\n",
    "        fig, ax = plt.subplots(figsize=(10, 24))\n",
    "\n",
    "        ax.set_title('Original')\n",
    "        ax.imshow(original[ix])\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_checker(original, ground_truth):\n",
    "    \"\"\"Shows original images in array with their ground truth masks\"\"\"\n",
    "    for _ in range(4):\n",
    "        ix = np.random.randint(0, original.shape[0])\n",
    "        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 24))\n",
    "\n",
    "        ax1.set_title('Original')\n",
    "        ax1.imshow(original[ix])\n",
    "        ax1.axis('off')\n",
    "\n",
    "        ax2.set_title('Ground Truth')\n",
    "        ax2.imshow(np.squeeze(ground_truth[ix]))\n",
    "        ax2.axis('off')    \n",
    "\n",
    "        plt.subplots_adjust(wspace=.3, hspace=.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(pretrained_model=False, model_name=model_name):\n",
    "    '''\n",
    "    Creates a new U-Net model\n",
    "    '''\n",
    "    inputs = Input((X_train.shape[1], X_train.shape[2], 3))\n",
    "    s = Lambda(lambda x: x) (inputs) # removed / 255\n",
    "\n",
    "    c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\n",
    "    c1 = Dropout(0.1) (c1)\n",
    "    c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "    c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
    "    c2 = Dropout(0.1) (c2)\n",
    "    c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "    c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
    "    c3 = Dropout(0.2) (c3)\n",
    "    c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "    c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
    "    c4 = Dropout(0.2) (c4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "    c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n",
    "    c5 = Dropout(0.3) (c5)\n",
    "    c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n",
    "\n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
    "    c6 = Dropout(0.2) (c6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
    "    c7 = Dropout(0.2) (c7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n",
    "    c8 = Dropout(0.1) (c8)\n",
    "    c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n",
    "    c9 = Dropout(0.1) (c9)\n",
    "    c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(original, predicted, predicted_mask, ground_truth=None, repeat=False):\n",
    "    \"\"\"\n",
    "    Plots the original image, predicted image, mask from the predicted image, and ground truth mask.\n",
    "    ground_truth: None for testing images without masks\n",
    "    repeat: use the same first 4 images in the dataset for comparison\n",
    "    \"\"\"\n",
    "    ncols_calc = 3\n",
    "    if ground_truth is not None:\n",
    "        ncols_calc = 4\n",
    "        \n",
    "    \n",
    "    for i in range(4):\n",
    "        if repeat:\n",
    "            ix = i\n",
    "        else:\n",
    "            ix = np.random.randint(0, predicted.shape[0])\n",
    "            \n",
    "        fig, ax = plt.subplots(ncols=ncols_calc, figsize=(ncols_calc*5, 24))\n",
    "\n",
    "        ax[0].set_title('Original')\n",
    "        ax[0].imshow(original[ix])\n",
    "        ax[0].axis('off')\n",
    "\n",
    "        ax[1].set_title('Predicted')\n",
    "        ax[1].imshow(np.squeeze(predicted[ix]))\n",
    "        ax[1].axis('off')    \n",
    "\n",
    "        ax[2].set_title('Predicted Mask')\n",
    "        ax[2].imshow(np.squeeze(predicted_mask[ix]))\n",
    "        ax[2].axis('off')\n",
    "        \n",
    "        if ground_truth is not None:\n",
    "            ax[3].set_title('Ground Truth')\n",
    "            ax[3].imshow(np.squeeze(ground_truth[ix]))\n",
    "            ax[3].axis('off')\n",
    "\n",
    "        plt.subplots_adjust(wspace=.3, hspace=.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_sets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bb437e9f78ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_train_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Batch Number {batch_number} of {len(training_sets)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_sets' is not defined"
     ]
    }
   ],
   "source": [
    "# ========================================================== SPLIT AND BATCH THE IMAGES\n",
    "# ========================================================== SPLIT AND BATCH THE IMAGES\n",
    "# ========================================================== SPLIT AND BATCH THE IMAGES\n",
    "\n",
    "for batch_number, current_train_set in enumerate(training_sets, start=1):\n",
    "\n",
    "    print(f'Batch Number {batch_number} of {len(training_sets)}')\n",
    "\n",
    "    for current_image in current_train_set:\n",
    "\n",
    "        x = load_batch(train_image_dir)\n",
    "        y = load_batch(train_mask_dir)\n",
    "\n",
    "        # debugging\n",
    "        # image_checker(original=x, ground_truth=y)\n",
    "\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x, y, \n",
    "                                                      random_state=23, \n",
    "                                                      test_size=train_val_split_size)\n",
    "\n",
    "    shape_and_mem(x_train)\n",
    "    shape_and_mem(y_train)\n",
    "\n",
    "    if pretrained_model or batch_number > 1:\n",
    "        print('Loading Trained Model')\n",
    "        model = load_model(model_path)\n",
    "    else:\n",
    "        print('Creating New Model')\n",
    "        model = make_model(pretrained_model=pretrained_model,\n",
    "                           model_name=model_name)\n",
    "\n",
    "    # set data_augmentation to True or False in the constants cell\n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        history = model.fit(x_train, y_train, batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1,\n",
    "                    workers=12,\n",
    "                    use_multiprocessing=True,\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    validation_steps=(x_train.shape[0] // batch_size) * train_val_split_size,\n",
    "                    callbacks=[early_stop, check_point, ValPlotCallback()])\n",
    "    else:\n",
    "        print('Using real-time data augmentation.')\n",
    "        # This will do preprocessing and realtime data augmentation:\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "            # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            rotation_range=0,\n",
    "            # randomly shift images horizontally (fraction of total width)\n",
    "            width_shift_range=0.1,\n",
    "            # randomly shift images vertically (fraction of total height)\n",
    "            height_shift_range=0.1,\n",
    "            shear_range=0.,  # set range for random shear\n",
    "            zoom_range=0.,  # set range for random zoom\n",
    "            channel_shift_range=0.,  # set range for random channel shifts\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode='nearest',\n",
    "            cval=0.,  # value used for fill_mode = \"constant\"\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False,  # randomly flip images\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale=None,\n",
    "            # set function that will be applied on each input\n",
    "            preprocessing_function=None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format=None,\n",
    "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "            validation_split=0.0)\n",
    "\n",
    "        # compute quantities required for feature-wise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "        datagen.fit(x_train)\n",
    "\n",
    "        # fit the model on the batches generated by datagen.flow()\n",
    "        history = model.fit_generator(\n",
    "                    datagen.flow(x_train, y_train,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs,\n",
    "                                workers=12,\n",
    "                                use_multiprocessing=True,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                verbose=1,\n",
    "                                steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                                validation_steps=(\n",
    "                                    x_train.shape[0] // batch_size) * train_val_split_size,\n",
    "                                callbacks=[early_stop, check_point, ValPlotCallback()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # save model and architecture to single file\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_val_pred = model.predict(X_val, verbose=1, batch_size=batch_size)\n",
    "\n",
    "model.evaluate(x=X_val, y=y_val, batch_size=batch_size)\n",
    "\n",
    "# simple threshold to change to 1/0, mask\n",
    "X_val_pred_mask = (X_val_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "plot_predictions(original=X_val, predicted=X_val_pred, predicted_mask=X_val_pred_mask, ground_truth=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.jeremyjordan.me/evaluating-image-segmentation-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [np.array(\n",
    "        image_splitter(\n",
    "            cv2.imread(TEST_IMAGE_DIR + img_name).astype(np.uint8), \n",
    "            num_col_splits=split_cols, \n",
    "            num_row_splits=split_rows,\n",
    "            resize=True,\n",
    "            resize_height=img_resize_height,\n",
    "            resize_width=img_resize_width\n",
    "        )\n",
    "    ) for img_name in tqdm(test_filenames[:10])]\n",
    "\n",
    "X_test = (np.vstack(X_test)/255).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_and_mem(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_and_mem(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mask = (y_pred > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_predictions(original=X_test, predicted=X_test_pred, predicted_mask=X_test_pred_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# result = cv2.bitwise_and(test_split[0], test_split[0], mask=prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
